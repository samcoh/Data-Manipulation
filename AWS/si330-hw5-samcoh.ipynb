{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI 330: Homework 5: APIs on AWS\n",
    "\n",
    "\n",
    "## Due: Friday, October 19, 2018,  11:59:00pm\n",
    "\n",
    "### Submission instructions\n",
    "After completing this homework, you will turn in two files via Canvas ->  Assignments -> HW 5:\n",
    "- Your Notebook, named si330-hw5-YOUR_UNIQUE_NAME.ipynb and\n",
    "- the HTML file, named si330-hw5-YOUR_UNIQUE_NAME.html.\n",
    "\n",
    "### Name:  Samantha Cohen\n",
    "### Uniqname: samcoh\n",
    "### People you worked with: Rhea, Emil, and Will \n",
    "## Top-Level Goal\n",
    "To create a microservice that returns the counts of all bigrams in a text passage.\n",
    "\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "After completing this Lab, you should know how to:\n",
    "* create an AWS Lambda function that takes a string and returns the counts of all bigrams in that text\n",
    "* write an AWS API Gateway integration which allows both GET and POST requests to access an AWS Lambda\n",
    "* write documenation to the microservice that you've created\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of Steps For Analysis\n",
    "Here's an overview of the steps that you'll need to do to complete this lab.\n",
    "2. Upload data to an S3 bucket\n",
    "1. Create an AWS Lambda function that normalizes, tokenizes, and creates and counts bigrams from text, both via a POST request with the text and via a GET request to a URL that returns the text (e.g. an S3 bucket)\n",
    "3. Create a python code block in this notebook to demonstrate the functionality of your microservice\n",
    "\n",
    "Each of these steps is detailed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Upload data to an S3 bucket\n",
    "To get ready to test the POST functionality of the code you generate in the next step, you should upload a text file that is **500 or fewer lines** to an S3 bucket.  See the [description of cross-origin resource sharing (CORS)](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) for an explanation of why we want to put the data in the same domain (`amazonaws.com`) as the Lambda.\n",
    "\n",
    "Follow the same approach that we used in the lab to upload a small text file to your S3 bucket, ensuring that the permissions are set to allow public access\n",
    "\n",
    "### Q1: Enter the URL of your text file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My URL: https://s3.amazonaws.com/si330f18-hw5-samcoh/write-up.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create an AWS Lambda function that normalizes, tokenizes, and creates and counts bigrams of words from text\n",
    "\n",
    "Similar to what we did in the lab, you're going to create a microservice that consists of two parts: an AWS Lambda and an API Gateway.  You can use exactly the same technique that we did in the lab to get started.\n",
    "\n",
    "You will need to modify the code in the Lambda to handle two types of requests:\n",
    "1. A GET request with a queryStringParameter of `url=http://some.url.goes.here/text.txt`, which specifies the location of the text to be processed; AND\n",
    "2. A POST request with the text to be processed included as the `\"text\"` value in the body payload.\n",
    "\n",
    "Both types of requests should return a response whose body contains a JSON-encoded dictionary with a key called `bigram_counts`, which should contain a dictionary mapping each bigram onto the count of how many times it appears in the text. **NOTE:** Because JSON objects can only have keys that are strings, you should encode bigrams as the two words separated by a single space, not as tuples.\n",
    "\n",
    "For example, if the following text were given to this microservice:\n",
    "\n",
    "> The quick brown fox jumps over the quick brown squirrel.\n",
    "\n",
    "It should return a response whose `body` is a JSON-encoded version of this dictionary:\n",
    "\n",
    "```javascript\n",
    "{\n",
    "  \"bigram_counts\": {\n",
    "    \"the quick\": 2,\n",
    "    \"quick brown\": 2,\n",
    "    \"brown fox\": 1,\n",
    "    \"fox jumped\": 1,\n",
    "    \"jumped over\": 1,\n",
    "    \"over the\": 1,\n",
    "    \"brown squirrel\": 1\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "### The following code block is a reasonable starting point for creating your Lambda.  Note that this code should not be run in this notebook but rather serve as the starting point for your work in the Lambda editor.\n",
    "\n",
    "**NOTE** Please see https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python for hints about how to create bigrams without NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PUT SOME DOCUMENTATION HERE\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from botocore.vendored import requests # We've added this line in addition to what was in the lab.\n",
    "# You'll need to figure out how to use this `requests` module yourself --- it works similar to the \n",
    "# `requests` module that you've used before. \n",
    "    \n",
    "def lambda_handler(event, context):\n",
    "    method = event['httpMethod']\n",
    "\n",
    "    text = ''\n",
    "    # d holds our response variables\n",
    "    d = {\"bigram_counts\": {}}\n",
    "    \n",
    "    if method == 'GET':\n",
    "        # handle GET method\n",
    "        params = event['queryStringParameters']\n",
    "        if params:\n",
    "            response = requests.get(\"https://s3.amazonaws.com/si330f18-hw5-samcoh/write-up.txt\")\n",
    "            url = response.text # retrieve the text from the URL\n",
    "    if method == 'POST':\n",
    "        # handle POST method\n",
    "        body = json.loads(event['body'])\n",
    "        if 'text' in body:\n",
    "            url = body['text']\n",
    "            # do something \n",
    "            #pass\n",
    "    text = url.strip().split(\".\")\n",
    "    normalized_text = [x.strip().lower() for x in text if x != \"\"]\n",
    "    tokenized_words = [] \n",
    "    for word in normalized_text:\n",
    "        w = re.compile('\\w+').findall(word)\n",
    "        tokenized_words.append(\" \".join(w)) \n",
    "    #print(tokenized_words)\n",
    "    bigrams = [b for l in tokenized_words for b in zip(l.strip().split(\" \")[:-1], l.strip().split(\" \")[1:])]\n",
    "    for first,second in bigrams: \n",
    "        combined = first +\" \"+ second\n",
    "        if combined not in d[\"bigram_counts\"]:\n",
    "            d[\"bigram_counts\"][combined] = 0 \n",
    "        d[\"bigram_counts\"][combined] += 1 \n",
    "    #print(bigrams)\n",
    "    #normalized_text = [x.strip().lower() for x in text]\n",
    "    #print(normalized_text)\n",
    "    #text = url.strip().split(\" \")\n",
    "    #normalized_text = [x.strip().lower() for x in text if x != \"\"]\n",
    "    #tokenized_words = [] \n",
    "    #for word in normalized_text:\n",
    "        #w = re.compile('\\w+').search(word).group(0)\n",
    "        #tokenized_words.append(w) \n",
    "    \n",
    "    #normalized_text = [x.strip().lower() for x in url.strip().split(\".\")]\n",
    "            \n",
    "    # 1. normalize\n",
    "    \n",
    "    # 2. tokenize\n",
    "    # NOTE: there are many ways to do this depending on your definition of \"word\", \n",
    "    # they may yield slightly different results --- that is okay. The method below\n",
    "    # has the nice property that words with apostrophes in them are not broken up.\n",
    "    \n",
    "    # 3. find bigrams\n",
    "    # NOTE: see https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python\n",
    "    #       for hints about how to create bigrams without nltk\n",
    "    \n",
    "    # 4. count bigrams\n",
    "    \n",
    "    # 5. return response\n",
    "    # Note the strict format of the return dictionary\n",
    "    # It must contain these three elements, and the body\n",
    "    # must be a stringified JSON object (i.e. you have to call \n",
    "    # json.dumps on the JSON structure you're returning)\n",
    "    return { \n",
    "        \"statusCode\": 200,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"body\": json.dumps(d),\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1: Enter the URL of your Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your Lambda's URL here: https://dfnz7e4j1m.execute-api.us-east-1.amazonaws.com/default/count_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2: Copy your final Lambda code into the following code block (but do not run it here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module normalizes and tokenizes words and returns a dictionary of bigram counts. \n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from botocore.vendored import requests # We've added this line in addition to what was in the lab.\n",
    "# You'll need to figure out how to use this `requests` module yourself --- it works similar to the \n",
    "# `requests` module that you've used before.\n",
    "\n",
    "    \n",
    "def lambda_handler(event, context):\n",
    "    method = event['httpMethod']#alwYS -event object; will tell you if get or post \n",
    "\n",
    "    text = ''\n",
    "    # d holds our response variables\n",
    "    d = {\"bigram_counts\": {}}\n",
    "    \n",
    "    if method == 'GET':\n",
    "        # handle GET method\n",
    "        params = event['queryStringParameters'] #always same \n",
    "        if params:\n",
    "            url = requests.get(params[\"url\"]).text #passed in url \n",
    "            text = url \n",
    "            \n",
    "    if method == 'POST':\n",
    "        # handle POST method\n",
    "        body = json.loads(event['body']) #akwats sane \n",
    "    \n",
    "        if 'text' in body:\n",
    "            text = body[\"text\"]#the key varries\n",
    "    \n",
    "    # 1. normalize      \n",
    "    \n",
    "    normalized_text = text.strip().lower().split()\n",
    "    \n",
    "    # 2. tokenize\n",
    "    # NOTE: there are many ways to do this depending on your definition of \"word\", \n",
    "    # they may yield slightly different results --- that is okay. The method below\n",
    "    # has the nice property that words with apostrophes in them are not broken up.\n",
    "    \n",
    "    tokenized_words = [] \n",
    "    for word in normalized_text:\n",
    "        w = re.compile('[\\w-]+').findall(word)\n",
    "        tokenized_words.append(w[0]) \n",
    "    \n",
    "    # 3. find bigrams\n",
    "    # NOTE: see https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python\n",
    "    #       for hints about how to create bigrams without nltk\n",
    "    bigrams = list(zip(tokenized_words[:-1], tokenized_words[1:]))\n",
    "    \n",
    "    # 4. count bigrams\n",
    "    for first,second in bigrams: \n",
    "        combined = first +\" \"+ second\n",
    "        if combined not in d[\"bigram_counts\"]:\n",
    "            d[\"bigram_counts\"][combined] = 0 \n",
    "        d[\"bigram_counts\"][combined] += 1 \n",
    "        \n",
    "    # 5. return response\n",
    "    # Note the strict format of the return dictionary\n",
    "    # It must contain these three elements, and the body\n",
    "    # must be a stringified JSON object (i.e. you have to call \n",
    "    # json.dumps on the JSON structure you're returning)\n",
    "    return { \n",
    "        \"statusCode\": 200,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"body\": json.dumps(d),\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Demonstrate the GET and POST functionality of your Lambda\n",
    "\n",
    "### Q3.1: Create a code block that uses `requests` to demonstrate the `GET` functionality of your Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://dfnz7e4j1m.execute-api.us-east-1.amazonaws.com/default/count_bigrams\"\n",
    "response = requests.get(url,params = {\"url\": \"https://s3.amazonaws.com/si330f18-hw5-samcoh/write-up.txt\"})\n",
    "\n",
    "print(response.headers)\n",
    "print()\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2: Create a code block that uses `requests` to demonstrate the `POST` functionality of your Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': 'Wed, 07 Nov 2018 18:10:08 GMT', 'Content-Type': 'application/json', 'Content-Length': '140', 'Connection': 'keep-alive', 'x-amzn-RequestId': '5c2fdb85-e2b8-11e8-b158-d5536e1ec0cb', 'x-amz-apigw-id': 'QAOUDFIuoAMFbVA=', 'X-Amzn-Trace-Id': 'Root=1-5be32a80-607fdc8498474378d97471c0;Sampled=0'}\n",
      "\n",
      "{\"bigram_counts\": {\"the quick\": 2, \"quick brown\": 2, \"brown fox\": 1, \"fox jumped\": 1, \"jumped over\": 1, \"over the\": 1, \"brown squirrel\": 1}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json \n",
    "url = \"https://dfnz7e4j1m.execute-api.us-east-1.amazonaws.com/default/count_bigrams\"\n",
    "response = requests.post(url, data = json.dumps({\"text\": \"The quick brown fox jumped over the quick brown squirrel.\"}))\n",
    "#this post aws lambda function make post expects json formatted dictionary keys text value of that elelment is what you want to parse \n",
    "print(response.headers)\n",
    "print()\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS\n",
    "\n",
    "**BONUS.1** Break the microservice into three separate ones (normalizing, tokenizing, and counting bigrams). Paste your code for each one here (clearly labelled), followed by a code block that executes them in succession, passing the results from each previous step into the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting bigrams Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code using all three Lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BONUS.2** Re-write the last step in **BONUS.1** to generate arbitrary *n*-grams instead of bigrams. This microservice should take an additional parameter, `n`, which determines what size of *n*-gram it will count. Instead of returning a single value (`\"bigram_counts\"`), it should now return two values: `\"counts\"` (the dictionary of n-gram counts), and `\"n\"`, the value of `n` input into the microservice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code using ngram Lambda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
